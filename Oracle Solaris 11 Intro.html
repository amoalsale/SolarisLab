<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<title>Introduction to Solaris 11</title>
<meta name="generator" content="Bluefish 2.0.2" >
<meta name="author" content="Pavel Anni" >
<meta name="date" content="2011-12-19T17:25:01-0500" >
<meta name="copyright" content="">
<meta name="keywords" content="">
<meta name="description" content="">
<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta http-equiv="content-type" content="application/xhtml+xml; charset=UTF-8">
<meta http-equiv="content-style-type" content="text/css">
<meta http-equiv="expires" content="0">

<style type="text/css">
p {
	line-height:140%;
}
kbd {
	font-family: courier, fixed, monospace;
	font-weight: bold;
}
pre {
	font-weight: normal;
	font-family: courier, fixed, monospace;
	color: black;
	line-height:140%;
}
code {
	font-weight: normal;
	font-family: courier, fixed, monospace;
}

body {
	font-family: arial, helvetica, sans-serif;
	font-size: 0.875em;
}

table
{
border:1px solid grey;
border-collapse:collapse;
width:80%;
}
th {
background-color:grey;
color:white;
padding:4px 7px 4px 7px;
border: 1px solid grey;
border-collapse:collapse;
}

td {
padding:4px 7px 4px 7px;
border: 1px solid grey;
border-collapse:collapse;
}


</style>

</head>
<body>


<h1>An Introduction to Oracle Solaris 11</h1> 

<h3><em>Author: Pavel Anni, Oracle</em></h3>
<h4><em>pavel.anni@oracle.com</em></h4><h4><em>December 2011</em></h4>
<h2>Lab Introduction </h2><p>In this lab we introduce the most interesting features of Solaris 11, based on the real life use cases. We will: </p><ul>
<li>create a pool and a file system with ZFS; expand it; snapshot and clone it; use ZFS compression and deduplication; </li>
<li>create and boot a new boot environment, make some fatal mistakes and reboot back into the original one;</li><li>create a couple of zones; install some applications into them; clone a zone; use Resource Management with the zones; </li><li>use DTrace to find bottlenecks in the system</li> 
</ul>

<h2>Prerequisites </h2><p>This lab requires access to Solaris 11 system either in VirtualBox virtual machine or in Oracle Solution Center cloud. </p>

<h3>Using the Oracle Solution Center cloud installation</h3> <p>
Use your OSC instructions. You will be provided with IP addresses, access instructions and login credentials by your OSC instructor.</p> <h3>Using Your Machine</h3> <p>If you're performing this lab on your own machine, we've provided a VirtualBox appliance that contains 
all the software and configuration necessary to complete this lab. 
All you need is VirtualBox and a modern (with a CPU supporting virtualization, AMD-V or VT-x) laptop/desktop with at least 3 GB of RAM. </p><ul>
<li>Download and install the latest version of VirtualBox (http://www.virtualbox.org/wiki/Downloads). </li><li>Download or find on the provided DVD and import the Hands On Lab machine into VirtualBox (File > Import Appliance). You will have
to accept the OTN Oracle Solaris license to use the appliance. </li></ul>

<p>The Hands On Lab virtual appliance contains the following: </p><ul>
<li>Solaris 11 installation configured for this lab</li> <li>This lab document</li> 
</ul>
<h2>Lab Outline </h2><table>

<tr><th>#</th> <th>Topic (Use Case)</th> <th>Minutes</th> <th>Lab</th></tr> 
<tr><td>1</td> <td>You have some disks to use for your new file system. Create a new disk pool and a file system on top of it.</td> <td>10</td> <td>ZFS Pools</td> </tr>
<tr><td>2</td> <td>You have to create home directories for your users; use file system quota to limit their space.</td> 
<td>10</td> <td>ZFS File systems</td> </tr>
<tr><td>3</td> <td>You are becoming low on your disk space. Add a couple more disks to your pool and expand your file system. What other ZFS features can help you to save space in the future?</td> 
<td>15</td> <td>ZFS Compression</td></tr> <tr><td>4</td> <td>Users tend to keep a lot of similar files in their archives. Is it possible to save space by using 
 deduplication?</td> 
<td>10</td> <td>ZFS Deduplication</td></tr><tr><td>5</td> <td>A user has accidentally deleted her file. How to restore it without getting to the backup?</td> 
<td>10</td> <td>ZFS Snapshots</td> </tr>
<tr><td>6</td> <td>You want to make updates to the system, but you want to be able to resturn back to the previous state.</td> 
<td>10</td> <td>Boot Environments</td></tr>
<tr><td>7</td> <td>Your development team wants a separate environment to develop their application.</td> <td>20</td> <td>Zones</td> </tr>
<tr><td>8</td> <td>You have to install some application packages in the zone and create users.</td> <td>15</td> <td>Inside a Zone</td> </tr>
<tr><td>9</td> <td>Your development team wants a copy of this environment.</td> <td>10</td> <td>Zone Cloning</td> </tr>
<tr><td>10</td> <td>Your departments want to know how much resources do they use to pay their fair share for the datacenter infrastructure.</td> <td>10</td> <td>Zone Monitoring</td> </tr>
<tr><td>11</td> <td>You want to control the zones' resource usage. You want to assign certain amount of processing power to each zone.</td> <td>15</td> <td>Resource Management</td> </tr>
<tr><td>12</td> <td>You have noticed that system utilization is very high. How to find the process which consumes most of the resources?</td> <td>15</td> <td>DTrace CPU</td></tr> 
<tr><td>13</td> <td>You have noticed that free storage space has decreased dramatically and keeps decreasing very fast. Who is "eating" our disk space?</td> <td>10</td> <td>DTrace Disk </td></tr>
<tr><td>14</td> <td>How can all this help you in building next generation, cloud oriented infrastructure?</td> 
<td>10</td> <td>Putting It All Together</td> </tr>
</table>
<h2>The Environment</h2> <p>In this lab we are going to use Oracle Solaris 11 virtual appliance in Oracle VirtualBox environment. 
If you are using lab machines, the appliance is already installed. You can also download the appliance 
from Oracle Technology Network. </p>
<p>By default, VirtualBox assigns the IP address 10.0.2.15 to the Solaris global zone. 
We will be using also IP addresses 10.0.2.16 and 10.0.2.17 for local zones. As we are using VirtualBox in NAT (network address translation) mode, 
this shouldn't interfere with your outside network environment. </p>

<p>If you are using OSC virtual machines, you will be provided with IP addresses to use in this lab.</p><p>You should login into Solaris desktop with the following credentials: </p>
<p>Username: <code><strong>lab</strong></code> Password: <code><strong>oracle1</strong></code> </p><p>After logging in, open a terminal window and assume the root role: </p><pre>lab@solaris:~$ <strong>su root</strong> </pre>
<p>Password for root is: <code><strong>oracle2011</strong></code>. Don't use the dash "-" in this command: 
we are going to keep the user <code>lab</code>'s environment settings </p><p>Note: we don't recommend to log in as <code>root</code>. In Solaris 11 it is prohibited by default; 
<code>root</code> is only a role, not a login name.</p><h2>Exercise 1: ZFS Pools </h2><p><strong>Task:</strong> You have several disks to use for your new file system. 
Create a new disk pool and a file system on top of it. </p><p><strong>Lab:</strong> We will check the status of disk pools, create our own pool and expand it.</p>

<p>Our Solaris 11 installation already has a ZFS pool. It's your root file system. Check this: </p>

<pre>root@solaris:~# <kbd>zpool list</kbd> </pre>

<pre>
NAME    SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
rpool  15.9G  5.64G  10.2G  35%  1.00x  ONLINE  -
</pre><p>What do we know about this pool? </p><pre>root@solaris:~# <kbd>zpool status rpool </kbd>  pool: rpool
 state: ONLINE
  scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	rpool       ONLINE       0     0     0
	  c3t0d0s0  ONLINE       0     0     0

errors: No known data errors</pre><p>Let's now create our own ZFS pool. What do we need for that? Just several disks and one command. 
Check your /dev/dsk directory; we've created there several files that act as disks in our lab: </p><pre>root@solaris:~# <kbd>ls /dev/dsk/disk*</kbd> /dev/dsk/disk0 /dev/dsk/disk2 /dev/dsk/disk4 /dev/dsk/disk6 /dev/dsk/disk8 /dev/dsk/disk1 /dev/dsk/disk3 /dev/dsk/disk5 /dev/dsk/disk7 /dev/dsk/disk9 </pre>

<p>If the files are not there (e.g. you are running this lab on your own Solaris 11 installation) then create them:</p>
<pre>
root@solaris:~# <kbd>mkfile 200m disk0 disk1 disk2 disk3 disk4 disk5 disk6 disk7 disk8 disk9</kbd>
</pre>
<p>We'll take four disks and create a ZFS pool with RAID-Z protection: </p><pre>root@solaris:~# <kbd>zpool create labpool raidz disk0 disk1 disk2 disk3 </kbd></pre><p>That was easy, wasn't it? And fast, too! Check our ZFS pools again: </p><pre>root@solaris:~# <kbd>zpool list</kbd> NAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
labpool   780M   194K   780M   0%  1.00x  ONLINE  -
rpool    15.9G  7.57G  8.30G  47%  1.00x  ONLINE  -</pre>

<p>By the way, the file system was also created and mounted automatically: </p>
<pre>root@solaris:~# <kbd>zfs list labpool</kbd> NAME                     USED  AVAIL  REFER  MOUNTPOINT
labpool                 97.2K   551M  44.9K  /labpool</pre><p>Do you need more space? Adding disks to the existing ZFS pool is as easy as creating it: </p><pre>root@solaris:~# <kbd>zpool add labpool raidz disk4 disk5 disk6 disk7</kbd> </pre><p>Check it again: </p><pre>root@solaris:~# <kbd>zfs list labpool</kbd> NAME      USED  AVAIL  REFER  MOUNTPOINT
labpool  97.2K  1.11G  44.9K  /labpool</pre><p>Note the increased file system's size. </p><h2>Exercise 2: ZFS File Systems</h2> <p><strong>Task:</strong> You have to create home directories for your users; use file system quota to limit their space. </p><p><strong>Lab:</strong> We'll create a user "<code>joe</code>" and set a disk quota for him. </p>
<p>
Creating a user is pretty similar to most Unix/Linux systems. What's different is what's going on behind the scenes.
</p>
<pre>
root@solaris:~# <kbd>useradd -m joe</kbd> root@solaris:~# <kbd>passwd joe</kbd> 
New Password: <kbd>oracle1</kbd> 
Re-enter new Password: <kbd>oracle1</kbd>
passwd: password successfully changed for joe</pre><p>In Solaris 11 behind the scenes we create a <em>separate</em> ZFS file system for the user (parameter <code>-m</code>) 
in <code>/export/home</code> and mount it with AutoFS service at <code>/home/<em>username</em></code> when the user logs in.
Check it:</p>

<pre>
root@solaris:~# <kbd>zfs list</kbd>
NAME                     USED  AVAIL  REFER  MOUNTPOINT
labpool                 97.2K  1.11G  44.9K  /labpool
rpool                   7.65G  7.97G    39K  /rpool
rpool/ROOT              5.59G  7.97G    31K  legacy
rpool/ROOT/solaris      5.59G  7.97G  5.17G  /
rpool/ROOT/solaris/var   330M  7.97G   183M  /var
rpool/dump              1.03G  8.01G  1.00G  -
rpool/export            1.48M  7.97G    32K  /export
rpool/export/home       1.44M  7.97G    33K  /export/home
rpool/export/home/joe    686K  7.97G   686K  /export/home/joe
rpool/export/home/lab    760K  7.97G   760K  /export/home/lab
rpool/swap              1.03G  8.01G  1.00G  -
</pre>

<p>
What does it mean for us, system administrators? That means we can use all kinds of ZFS features 
(compression, deduplication, encryption) on a per-user basis. We can create snapshots and perform 
rollbacks on a per-user basis. More about that later. Now we'll set a disk quota for <code>joe</code>'s home
directory.
</p>

<pre>
root@solaris:~# <kbd>zfs set quota=200m rpool/export/home/joe</kbd>
</pre>
<p>Now change user to "<code>joe</code>" and check how much space you can use: </p><pre>root@solaris:# <kbd>su - joe </kbd>joe@solaris$ <kbd>mkfile 100m file1</kbd> joe@solaris$ <kbd>mkfile 100m file2 </kbd></pre><p>First file was created OK, but with the second one we've got an error: "Disk quota exceeded". </p><p>Change the quota for <code>joe</code> in the other window and try again: </p><pre>root@solaris:~# <kbd>zfs set quota=300m rpool/export/home/joe</kbd> </pre> 
<p>
Then change back to <code>joe</code>'s window and try again:
</p>

<pre>joe@solaris$ <kbd>/usr/sbin/mkfile 100m file2</kbd> </pre><p>Success! As you can see, it's pretty easy to create and manage ZFS filesystems. Remember, by default Solaris 11 creates
 a separate ZFS file system for each user. </p><h2>Exercise 3: ZFS Compression </h2>
<p><strong>Task:</strong> You are becoming low on your disk space. Now you know how to add more disks to your pool 
and expand your file system. What other ZFS features can help you to solve this problem? </p><p><strong>Lab:</strong> In our lab we will compress our Solaris manuals directory and see if we are able to use it after that. 
Create a separate filesystem for this: </p><pre>root@solaris:~# <kbd>zfs create rpool/zman </kbd>root@solaris:~# <kbd>zfs list | grep zman </kbd>rpool/zman                31K  7.78G    31K  /rpool/zman
</pre><p>Set compression to "gzip" (there are options to gzip and other algorithms too--check the manual). </p>

<pre>root@solaris:~# <kbd>zfs set compression=gzip rpool/zman </kbd></pre><p>Copy our Solaris manuals there (it will take some time, be patient): </p><pre>root@solaris:~# <kbd>cp -rp /usr/share/man/* /rpool/zman/ </kbd></pre><p>Compare the sizes: </p>
<pre>root@solaris:~# <kbd>du -sh /usr/share/man /rpool/zman</kbd> 149M	/usr/share/man
  68M	/rpool/zman
 </pre><p>We just have saved about 55% of disk space. Not bad! Check if you are able to use the manuals after compression: </p>
<pre>root@solaris:~# <kbd>export MANPATH=/rpool/zman</kbd> root@solaris:~# <kbd>man ls</kbd> </pre>

<h2>Exercise 4: ZFS Deduplication</h2>

<p><strong>Task:</strong> Users tend to keep a lot of similar files in their archives. Is it possible to save 
space by deduplication?</p>

<p><strong>Lab: </strong>We will create a ZFS file system with deduplication turned on and see if it helps.</p>

<p>
Let's model the following situation: we have a file system which is used as an archive. 
We'll create separate file systems for each user and imagine that they store similar files there.  
</p>

<p>Remember we have created ZFS pool called <code>labpool</code> in the first exercise? 
If you have skipped that exercise, create it now:</p>

<pre>
root@solaris:~# <kbd>zpool create labpool raidz disk0 disk1 disk2 disk3</kbd> 
</pre>

<p>Create a file system with deduplication and compression:</p>

<pre>
root@solaris:~# <kbd>zfs create -o dedup=on -o compression=gzip labpool/archive</kbd>
</pre>

<p>Create users' file systems (we'll call them a, b, c, d for simplicity):</p>

<pre>
root@solaris:~# <kbd>zfs create labpool/archive/a</kbd>
root@solaris:~# <kbd>zfs create labpool/archive/b</kbd>
root@solaris:~# <kbd>zfs create labpool/archive/c</kbd>
root@solaris:~# <kbd>zfs create labpool/archive/d</kbd>
</pre>

<p>
Check their "dedup" parameter:
</p>

<pre>
root@solaris:~# <kbd>zfs get dedup labpool/archive/a</kbd>
NAME               PROPERTY  VALUE          SOURCE
labpool/archive/a  dedup     on             inherited from labpool/archive
</pre>

<p>Children file systems inherit parameters from their parents.</p>

<p>Create an archive from /usr/share/man, for example.</p>

<pre>
root@solaris:~# <kbd>tar czf /tmp/man.tar.gz /usr/share/man</kbd>
</pre>

<p>And copy it to four file systems we've just created. Don't forget to check deduplication rate after each copy.</p>

<pre>
root@solaris:~# <kbd>cd /labpool/archive</kbd>
root@solaris:/labpool/archive# <kbd>ls -lh /tmp/man.tar.gz </kbd>
-rw-r--r--   1 root     root         34M Nov 15 09:12 /tmp/man.tar.gz
root@solaris:/labpool/archive# <kbd>zpool list labpool</kbd>
NAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
labpool  1.52G  1.05M  1.52G   0%  1.00x  ONLINE  -
root@solaris:/labpool/archive# <kbd>cp /tmp/man.tar.gz a/</kbd>
root@solaris:/labpool/archive# <kbd>zpool list labpool</kbd>
NAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
labpool  1.52G  40.5M  1.48G   2%  1.00x  ONLINE  -
root@solaris:/labpool/archive# <kbd>cp /tmp/man.tar.gz b/</kbd>
root@solaris:/labpool/archive# <kbd>zpool list labpool</kbd>
NAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
labpool  1.52G  40.7M  1.48G   2%  2.00x  ONLINE  -
root@solaris:/labpool/archive# <kbd>cp /tmp/man.tar.gz c/</kbd>
root@solaris:/labpool/archive# <kbd>zpool list labpool</kbd>
NAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
labpool  1.52G  41.5M  1.48G   2%  3.00x  ONLINE  -
root@solaris:/labpool/archive# <kbd>cp /tmp/man.tar.gz d/</kbd>
root@solaris:/labpool/archive# <kbd>zpool list labpool</kbd>
NAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
labpool  1.52G  41.2M  1.48G   2%  4.00x  ONLINE  -
</pre>

<p>It might take a couple of seconds for ZFS to commit those changes and report the correct dedup ratio. 
Just repeat the command if you don't see the results listed above. </p>

<p>Remember, we set compression to "on" as well when we created the file system? Check the compression ratio:</p>

<pre>
root@solaris:/labpool/archive# <kbd>zfs get compressratio labpool/archive</kbd>
NAME             PROPERTY       VALUE  SOURCE
labpool/archive  compressratio  1.01x  -
</pre>

<p>The reason is simple: we placed in the file system files that are compressed already. 
Sometimes compression can save you some space, sometimes deduplication can help. </p>
<h2>Exercise 5: ZFS Snapshots</h2> <p><strong>Task:</strong> A user has accidentally deleted her file. How to restore it without getting to the backup?</p> <p><strong>Lab:</strong> The following command shows all the file systems with their snapshots: </p><pre><kbd>root@solaris:~# zfs list -r -t all rpool</kbd> NAME                             USED  AVAIL  REFER  MOUNTPOINT
rpool                           7.72G  7.92G    40K  /rpool
rpool/ROOT                      5.59G  7.92G    31K  legacy
rpool/ROOT/solaris              5.59G  7.92G  5.17G  /
rpool/ROOT/solaris@install      96.9M      -  2.99G  -
rpool/ROOT/solaris/var           330M  7.92G   183M  /var
rpool/ROOT/solaris/var@install   147M      -   302M  -
rpool/dump                      1.03G  7.95G  1.00G  -
rpool/export                     864K  7.92G    32K  /export
rpool/export/home                832K  7.92G    34K  /export/home
rpool/export/home/joe             35K  7.92G    35K  /export/home/joe
rpool/export/home/lab            763K  7.92G   763K  /export/home/lab
rpool/swap                      1.03G  7.95G  1.00G  -
rpool/zman                      63.0M  7.92G  63.0M  /rpool/zman
</pre><p>As you can see, in the freshly installed system we already have a snapshot of our original installation (<code>solaris@install</code>). Let's see what can be done with snapshots. Create a sample text file <code>first.txt</code> with a text editor (<code>gedit, vi</code>) or with a simple Solaris command: </p>
<pre>root@solaris:~# <kbd>echo "first line\nsecond line" > first.txt</kbd> root@solaris:~# <kbd>cat first.txt </kbd>first line second line </pre><pre>root@solaris:~# <kbd>zfs snapshot rpool/export/home/lab@snap1</kbd> root@solaris:~# <kbd>zfs list -r -t all rpool </kbd>NAME                             USED  AVAIL  REFER  MOUNTPOINT
rpool                           7.72G  7.92G    40K  /rpool
rpool/ROOT                      5.59G  7.92G    31K  legacy
rpool/ROOT/solaris              5.59G  7.92G  5.17G  /
rpool/ROOT/solaris@install      96.9M      -  2.99G  -
rpool/ROOT/solaris/var           330M  7.92G   183M  /var
rpool/ROOT/solaris/var@install   147M      -   302M  -
rpool/dump                      1.03G  7.95G  1.00G  -
rpool/export                     864K  7.92G    32K  /export
rpool/export/home                832K  7.92G    34K  /export/home
rpool/export/home/joe             35K  7.92G    35K  /export/home/joe
rpool/export/home/lab            763K  7.92G   763K  /export/home/lab
rpool/export/home/lab@snap1         0      -   763K  -
rpool/swap                      1.03G  7.95G  1.00G  -
rpool/zman                      63.0M  7.92G  63.0M  /rpool/zman
</pre><p>The snapshot uses 0 bytes because we have not changed anything in your home directory. Add a couple of sentences to our file "<code>first.txt</code>" and save it again.</p> <pre>root@solaris:~# <kbd>echo "third line\nfourth line" >> first.txt</kbd> </pre><p>How does it look like now? </p>
<pre>root@solaris:~# <kbd>cat first.txt </kbd>first line second line third line fourth line </pre><p>What if we wanted to restore our old content? Just roll it back: </p>
<pre>root@solaris:~# <kbd>zfs rollback rpool/export/home/lab@snap1</kbd> </pre><p>Check the file: </p><pre>root@solaris:~# <kbd>cat first.txt </kbd>first line second line </pre><p>It's simple, it's fast, it doesn't require much space. Actually, it doesn't require space at all until you have made some changes. </p>
<p>It should be noted that if you want to rollback not the latest snapshot, all more recent snapshots will be deleted 
(Solaris will warn you about that). Obviously, it's possible to make snapshots on a regular basis. 
You don't even have to use <code>cron(1M)</code> for that, there is a special package called <code>zfs-auto-snapshot</code>. </p><p>Food for thought: How can snapshots be used in the real life environment? Backup is the first idea that comes to mind. What else? </p>


<h2>Exercise 6: Boot Environments</h2>

<p><strong>Task:</strong> You want to make updates to your system, but you want to be able to return back to the previous state.</p>

<p><strong>Lab:</strong> We will use a Solaris 11 feature called Boot Environments. We'll create an extra boot environment, 
boot into it, modify it and then return back to our initial configuration if something went wrong.</p>

<p>The only command you want to know to work with Boot Environments is <code>beadm(1M)</code>. Start with 
showing all boot environments in the system:</p>

<pre>
root@solaris:~# <kbd>beadm list</kbd>
BE      Active Mountpoint Space Policy Created          
--      ------ ---------- ----- ------ -------          
solaris NR     /          6.87G static 2011-11-14 13:13 
</pre>

<p>Create a new boot environment:</p>

<pre>root@solaris:~# <kbd>beadm create solaris-new</kbd></pre>

<p>...And make it active for the next reboot:</p>

<pre>root@solaris:~# <kbd>beadm activate solaris-new</kbd></pre>

<p>Check the status again:</p>

<pre>
root@solaris:~# <kbd>beadm list</kbd>
BE          Active Mountpoint Space   Policy Created          
--          ------ ---------- -----   ------ -------          
solaris            /          126.14M static 2011-11-14 13:13 
solaris-new NR     -          7.13G   static 2011-11-15 11:09 
</pre>

<p>Now reboot your system (use desktop menu System->Shut Down...). 
After the system has rebooted you will see that your GRUB menu now has two items: "Oracle Solaris 11 11/11"
and "solaris-new". Boot the latter. </p>

<p>After the system has booted, pretend that you are the "Administrative Fascist" type of system administrator 
(see "Know your System Administrator" http://www.gnu.org/fun/jokes/know.your.sysadmin.html) and you have just found out
 that your users use the archive system on <code>labpool</code> to store their personal files. You have decided 
 to destroy the ZFS pool completely. </p>

<pre>
root@solaris:~# <kbd>zpool destroy labpool</kbd>
root@solaris:~# <kbd>zpool list</kbd>
NAME    SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
rpool  15.9G  5.64G  10.2G  35%  1.00x  ONLINE  -
</pre>

<p>No problem, no warning, all destroyed. But after 20 seconds you recalled that there were your favorite music, too. 
How to restore the pool?? Luckily, we have our previous boot environment saved. Just reboot and choose
"Oracle Solaris 11 11/11" instead of "solaris-new" in the GRUB menu. </p>

<p>After reboot first of all--check the pool:</p>

<pre>
root@solaris:~# <kbd>zpool list</kbd>
NAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
labpool  1.52G  40.7M  1.48G   2%  4.00x  ONLINE  -
rpool    15.9G  8.65G  7.22G  54%  1.09x  ONLINE  -
</pre>

<p>...And the filesystem:</p>

<pre>
root@solaris:~# <kbd>ls  /labpool</kbd>
archive
root@solaris:~# <kbd>ls -F /labpool/archive</kbd>
a/  b/  c/  d/
</pre>

<p>It seems everything is salvaged! Now check the boot environments status and make sure your current boot environment 
will be active after reboot.</p>

<pre>
root@solaris:~# <kbd>beadm list</kbd>
BE          Active Mountpoint Space   Policy Created          
--          ------ ---------- -----   ------ -------          
solaris     N      /          126.14M static 2011-11-14 13:13 
solaris-new R      -          7.13G   static 2011-11-15 11:09 
root@solaris:~# <kbd>beadm activate solaris</kbd>
root@solaris:~# <kbd>beadm list</kbd>
BE          Active Mountpoint Space   Policy Created          
--          ------ ---------- -----   ------ -------          
solaris     NR     /          7.10G   static 2011-11-14 13:13 
solaris-new -      -          143.92M static 2011-11-15 11:09 
</pre>

<p>Boot Environments have many applications: you can update your system, 
installing packages into inactive boot environment; create boot environment snapshots etc. We leave it
for your homework. </p>

<h2>Exercise 7: Zones </h2><p><strong>Task:</strong> Your development team wants a separate environment to develop their new application. </p><p><strong>Lab:</strong> We are going to use Solaris virtualization technology called Solaris Zones. First, we have 
to create a filesystem where all the zones will be located. In Solaris 11 it must be a ZFS filesystem. </p><pre>root@solaris:~# <kbd>zfs create -o compress=gzip -o dedup=on -o mountpoint=/zones rpool/zones</kbd> </pre>

<p>(Now we know how to save space with ZFS compression and dedup options!) </p>
<p>Check if it's created and mounted: </p><pre>root@solaris:~# <kbd>zfs list rpool/zones</kbd> NAME         USED  AVAIL  REFER  MOUNTPOINT
rpool/zones   31K  7.92G    31K  /zones</pre><p>Create a zone with a minimum set of parameters: <code>name</code> (in our case it will be <code>zone1</code>) and <code>zonepath</code>, 
where the zone's files will be located. </p><pre>root@solaris:~# <kbd>zonecfg -z zone1 </kbd>zone1: No such zone configured Use 'create' to begin configuring a new zone. zonecfg:zone1> <kbd>create</kbd> 
create: Using system default template 'SYSdefault'zonecfg:zone1> <kbd>set zonepath=/zones/zone1</kbd> zonecfg:zone1> <kbd>exit</kbd> 
</pre><p>As simple as that! By default, the zone will be configured with Exclusive-IP (that means it will use its own IP stack) and 
a VNIC will be automatically created for the zone. </p>
 
<p> To check the status of our newly created zone: </p>
<pre>root@solaris:~# <kbd>zoneadm list -cv </kbd>  ID NAME             STATUS     PATH                           BRAND    IP    
   0 global           running    /                              solaris  shared
   - zone1            configured /zones/zone1                   solaris  excl  
 </pre>
 
<p>The zone is configured, we can install and boot it right now. But before the installation we'll 
configure a profile for the Solaris instance which will be running inside the zone. 
We are trying to avoid configuring the zone interactively and have it ready for use
right after the first boot.</p>

<pre>
root@solaris:~# <kbd>sysconfig create-profile -o zone1-profile.xml</kbd>
</pre>

<p>
This command will bring you to the interactive dialog very similar to the standard Solaris installaion. 
You will have to enter:
<ul>
<li>Computer Name (hostname for the zone): <kbd>zone1</kbd></li>
<li>Network configuration: choose "<kbd>Automatically</kbd>"</li>
<li>Time zone: choose your time zone from the list</li>
<li>Date: confirm the current date</li>
<li>Root password: <kbd>oracle2011</kbd></li>
<li>New user account details: real name, login name and password. This will be the first user of the zone. 
We have entered "<kbd>Zone User</kbd>", "<kbd>zuser1</kbd>", "<kbd>oracle1</kbd>"</li>
</ul>
</p>

<p>
Now, when the zone's profile is created, we can install the zone and initialize it using this profile.
</p>

<pre>root@solaris:~# <kbd>zoneadm -z zone1 install -c /home/lab/zone1-profile.xml</kbd> A ZFS file system has been created for this zone.
Progress being logged to /var/log/zones/zoneadm.20111113T200358Z.zone1.install
       Image: Preparing at /zones/zone1/root.

 Install Log: /system/volatile/install.4418/install_log
 AI Manifest: /tmp/manifest.xml.NVaaNi
  SC Profile: /tmp/zone1-profile.xml
    Zonename: zone1
Installation: Starting ...</pre><p>Here you can take a break. The installation will take about 10 minutes, depending on your network connection.</p>
<pre> ...Long output is skipped... Next Steps: Boot the zone, then log into the zone console (zlogin -C) to complete the configuration process. </pre><p>Check the status again: </p><pre>root@solaris:~# <kbd>zoneadm list -cv</kbd>   ID NAME             STATUS     PATH                           BRAND    IP    
   0 global           running    /                              solaris  shared
   - zone1            installed  /zones/zone1                   solaris  excl  </pre>
<p>It's time to boot our zone: </p><pre>root@solaris:~# <kbd>zoneadm -z zone1 boot </kbd>root@solaris:~# <kbd>zoneadm list -cv</kbd>  ID NAME             STATUS     PATH                           BRAND    IP    
   0 global           running    /                              solaris  shared
   1 zone1            running    /zones/zone1                   solaris  excl  
</pre>
<p>
Note the zone's status has changed to "running".
</p>
<p>Now log into our zone's console (note -C). You will have to wait a couple of minutes while the 
system is initializing services for the first time. </p><pre>
root@solaris:~# zlogin -C zone1 [Connected to zone 'zone1' console] </pre><p>You will get the standard Solaris login prompt. Congratulations! You've just configured 
"virtualization within virtualization" using Oracle technologies: Solaris zones within Oracle VirtualBox. </p>

<p>
Try to login using <code>root</code>'s credentials (<code>root/oracle2011</code>). Here is the result:
</p>

<pre>
zone1 console login: root
Password: 
Roles can not login directly
Login incorrect
Nov 13 15:23:07 zone1 login: login account failure: Permission denied
</pre>

<p>
A-ha! This is a new Solaris 11 security feature called "root as a role". That means that you can't 
login into a system as "root".
You have to use normal user's credentials and only then you will be able to use "sudo" or "pfexec"
according to your roles and privileges. 
</p>

<p>
Try to login again with <code>zuser1/oracle1</code>.
</p>
<pre>
Oracle Corporation      SunOS 5.11      11.0    November 2011
zuser1@zone1:~$
</pre>

<p>Success!</p>
<p>Note: to escape from the zone's console use: ~. (tilde period). </p><h2>Exercise 8: Inside the Zone </h2><p><strong>Task:</strong> You have to install some application packages in the zone and create some users. </p><p><strong>Lab:</strong> Log in in the zone, create a user and install a web server application.</p> <pre>root@solaris:~# <kbd>zlogin zone1 </kbd>root@zone1:~# </pre><p>Play around with the usual sysadmin commands. How can you tell if you are in a zone or not? First, try <kbd>ps -ef</kbd>. 
Do you see anything unusual? Yes, you are right, the process IDs don't start with 0, but with some big number. 
Other than that, no visible difference between the normal Solaris installation and the zone. Try <kbd>uname -a, psrinfo, cat /etc/release</kbd> ... </p><!-- <p>IMPORTANT NOTE: To avoid network congestion during this lab, we have created a local package 
repository inside the virtual appliance. We have used it already during the zone's installation. 
If you are going to use this virtual appliance not only for this lab, please change the package repository settings to the default values. See Final Remarks at the end of this document.</p>  --><p>Now let's do something useful with the zone. Like running a web server, for example. Let's install and run Apache. </p>
<pre>root@zone1:~# <kbd>pkg list -a *apache*</kbd> . . .Skipped. . . web/server/apache-22 2.2.16-0.151.0.1 known ----. . .Skipped. . . root@zone1:~# <kbd>pkg install apache-22</kbd> . . .Skipped. . . </pre><p>We've installed it successfully, but it's not running yet. </p><pre>root@zone1:~# <kbd>svcs -a | grep apache</kbd> disabled 6:31:42 svc:/network/http:apache22 </pre><p>Start the Apache web server:</p> <pre>root@zone1:~# <kbd>svcadm enable apache22</kbd> root@zone1:~# svcs -a | grep apache online 6:34:03 svc:/network/http:apache22 </pre><p>Check if it's working from your global Solaris zone (your Solaris desktop): 
start Firefox and enter your zone's IP address into the URL field: 10.0.2.16. "It works!" -- the page usually reads. In our current VirtualBox 
configuration (with NAT networking) the zone is not visible from outside, but you can always try to change your VirtualBox configuration to 
Bridged networking and give your zone an IP address from your local network. (Do try this at home!). 
</p>
<p>Check if it's your zone who is talking. Go back to the zone's terminal 
window and change your web server homepage (I'm using vi here, as we don't have many choices in a freshly installed zone. If you are not 
familiar with vi, check our Vi Quick Reference below): </p>
<pre>root@zone1:~# <kbd>vi /var/apache2/2.2/htdocs/index.html </kbd></pre><p>Write here something like "This is Zone1 and it works!", save the file and 
reload the page in Firefox in your Solaris desktop. Did it work? Congratulations! </p><table width=50% cellpadding="0" cellspacing="0" border="0" bgcolor="lightgrey">
<tr><td>
<pre>Vi Quick Reference If you're unfamiliar with vi, following are a few common 
keyboard commands to get you through this exercise: k = up j = down w = right or forward one word b = left or back one word l = right 1 char h = left 1 char x = delete 1 char u = undo i = insert dd = delete entire current line esc = get out of edit mode :wq = write and quit 
:w! = write to a read-only file:q! = quite ignoring changes (do not write) </pre>
</td></tr></table><p>What else do we need? Try to create users in the zone. </p>
<pre>root@zone1:~# <kbd>useradd -m jack</kbd> root@zone1:~# <kbd>passwd jack</kbd>
New Password: <kbd>oracle1</kbd> (will not be displayed) Re-enter new Password: <kbd>oracle1</kbd> (will not be displayed) passwd: password successfully changed for jack root@zone1:~# <kbd>su - jack</kbd> Oracle Corporation	SunOS 5.11	11.0	November 2011
jack@zone1:~$ <kbd>ls</kbd>
local.cshrc    local.login    local.profile
jack@zone1:~$ </pre>
<p>Looks good! Try to login from your global zone (open another window on your Solaris desktop):</p><pre>lab@solaris:~$ <kbd>ssh -l jack 10.0.2.16</kbd></pre> 

<p>(It's a small letter L here, not the digit one) </p>
<p>For your homework: compare global and non-global zones installations. 
How many packages are installed in both? How many services are running? Check if you can login into the global zone with the zone user's (jack) credentials. 
Check if you can use your zone's root password in the global zone (of course, if they are different). </p><h2>Exercise 9: Zone Cloning </h2><p><strong>Task:</strong> Your development team wants a copy of this environment for testing purposes. </p><p><strong>Lab:</strong> We will configure a new zone ('zone2') and then clone it from the existing zone1.
<p>This time let's configure the zone in one line:</p>
<pre>root@solaris:~# <kbd>zonecfg -z zone2 "create; set zonepath=/zones/zone2; exit" </kbd></pre><p>Check:</p> <pre>root@solaris:~# <kbd>zoneadm list -cv</kbd>   ID NAME             STATUS     PATH                           BRAND    IP    
   0 global           running    /                              solaris  shared
   1 zone1            running    /zones/zone1                   solaris  excl  
   - zone2            configured /zones/zone2                   solaris  excl  
</pre>

<p>Before cloning we have to shutdown our running zone1:</p>
<pre>root@solaris:~# <kbd>zoneadm -z zone1 shutdown </kbd></pre><p>Then we create the new zone's profile. Start the System Configuration Tool and repeat 
all the configuration steps you did for zone1. Just change Computer Name to "zone2", 
user name to "zuser2" and password to "oracle2". </p>

<pre>
root@solaris:~# <kbd>sysconfig create-profile -o zone2-profile.xml</kbd>
</pre>
<p>Now clone zone1 and configure zone2 automatically using this profile: </p><pre>root@solaris:~# <kbd>zoneadm -z zone2 clone -c /home/lab/zone2-profile.xml zone1</kbd> root@solaris:~# <kbd>zoneadm list -cv </kbd>  ID NAME             STATUS     PATH                           BRAND    IP    
   0 global           running    /                              solaris  shared
   1 zone1            installed  /zones/zone1                   solaris  excl  
   2 zone2            installed  /zones/zone2                   solaris  excl  </pre>

<p>Now boot both zones:</p>

<pre>
root@solaris:~# <kbd>zoneadm -z zone1 boot</kbd> root@solaris:~# <kbd>zoneadm -z zone2 boot</kbd> root@solaris:~# <kbd>zoneadm list -cv </kbd>  ID NAME             STATUS     PATH                           BRAND    IP    
   0 global           running    /                              solaris  shared
   1 zone1            running    /zones/zone1                   solaris  excl  
   2 zone2            running    /zones/zone2                   solaris  excl  
</pre>
<p>Success! And it was faster than the initial installation, wasn't it? </p><p>After it's done, login into zone2. </p>
<p>root@solaris:~# <kbd>zlogin zone2 </kbd></p><p>First of all, what about our Apache server? </p><pre>root@zone2:~# <kbd>pkg list -a | grep apache</kbd> . . .Skipped. . . web/server/apache-22                              2.2.20-0.175.0.0.0.2.537   i--</pre><p>Great! It's installed already! Check if it's running: </p>
<pre>root@zone2:~# <kbd>svcs *apache*</kbd> online 11:48:47 svc:/network/http:apache22 </pre><p>Try the zone2 address (10.0.2.17) in Firefox in the global zone. </p>

<p>"This is Zone1 and it works!" - of course, we have cloned not only installed applications, but also 
their configurations. Change it to "Zone2", just for consistency sake.</p> <pre>root@zone2:~# <kbd>vi /var/apache2/2.2/htdocs/index.html </kbd></pre><h2>Exercise 10: Zone Monitoring </h2><p><strong>Task:</strong> Your departments want to know how much resources do they use to pay their fair 
share for the datacenter infrastructure.</p> <p><strong>Lab:</strong> Some familiar Solaris commands now include a -Z parameter to help you to 
monitor zones behavior. Try ps -efZ and prstat -Z to take a look. Try also a new command 
zonestat to show zone statistics. </p>

<pre>root@solaris:~# <kbd>zonestat -z zone1,zone2 5</kbd> </pre>

<pre>
Collecting data for first interval...
Interval: 1, Duration: 0:00:05
SUMMARY                   Cpus/Online: 1/1   PhysMem: 2047M  VirtMem: 3071M
                    ---CPU----  --PhysMem-- --VirtMem-- --PhysNet--
               ZONE  USED %PART  USED %USED  USED %USED PBYTE %PUSE
            [total]  0.05 5.45%  968M 47.3% 1251M 40.7%     0 0.00%
           [system]  0.01 1.51%  287M 14.0%  735M 23.9%     -     -
              zone1  0.00 0.16% 73.8M 3.60% 66.3M 2.16%     0 0.00%
              zone2  0.00 0.13% 73.9M 3.61% 67.2M 2.18%     0 0.00%
</pre>

<p>Note the parameters you can observe with <code>zonestat</code>: CPU utilization, physical and virtual memory usage,
 network bandwidth utilization. </p><h2>Exercise 11: Resource Management </h2><p><strong>Task:</strong> You want to control the zones' resource usage. You want to assign certain 
amount of processing power to each zone. </p><p><strong>Lab:</strong> We now know how to create and clone zones. Now let's try to cap CPU resources 
in one zone to demonstrate the basics of resource management in Solaris. </p>
<p>First, run a simple CPU-consuming script in the zone1: </p><pre>root@solaris:~# <kbd>zlogin zone1 "bash -c 'while true ; do date > /dev/null ; done'"</kbd> </pre><p>Note that we are simply using <code>zlogin</code> to pass the command to the zone. </p>
<p>What's going on in the global zone? Open another window, become root and check:</p><pre>root@solaris:~# <kbd>vmstat 5 </kbd></pre><p>Idle is 0, system time is around 70%. Not good.</p> <pre>root@solaris:~# <kbd>zonestat 5</kbd> </pre><p>Zone1 consumes 70-80% of total resources, the rest is spent in global zone (most likely serving zone1's requests). We decided to reduce the zone1's resource consumption and give it only 50% of our CPU cycles. </p><pre>root@solaris:~# <kbd>zonecfg -z zone1</kbd> zonecfg:zone1> <kbd>add capped-cpu</kbd> zonecfg:zone1:capped-cpu> <kbd>set ncpus=0.5 </kbd>zonecfg:zone1:capped-cpu> <kbd>end </kbd>zonecfg:zone1> <kbd>exit</kbd> root@solaris:~# <kbd>zoneadm -z zone1 reboot </kbd></pre><p>After zone1 reboots, log in into it again and repeat the same steps: run the bash script 
mentioned above and then run <code>zonestat 5</code> in another window. </p>

<pre>root@solaris:~# <kbd>zlogin zone1 "bash -c 'while true ; do date > /dev/null ; done'"</kbd> </pre>
<pre>root@solaris:~#<kbd> zonestat 5</kbd>
</pre>

<p>Do you see the difference? 
Are you happy with the result? </p><p>Is it also possible to change this CPU cap parameter on the fly: </p>
<pre>root@solaris:~# <kbd>prctl -n zone.cpu-cap -r -v 25 -i zone zone1 </kbd></pre><p>Check if it works: </p><pre>root@solaris:~# <kbd>zonestat 5 </kbd></pre><p>In your virtual appliance there is a simple tool which allows you to visualize the zones workloads. 
It simply pipes the zonestat output into the popular open source program gnuplot. Try this: </p><pre>root@solaris:~# <kbd>zoneplot</kbd></pre> <p>Don't forget to stop the infinite loop in your zone! Or simply halt the zone. </p><pre>root@solaris:~# <kbd>zoneadm -z zone1 halt</kbd> </pre><p>Other resources can be capped this way as well: memory, swap, number of threads etc. Again, think about how it can be used in real life situations? </p><h2>Exercise 12: DTrace CPU </h2><p><strong>Task:</strong> You have noticed that system utilization is very high. How to find the 
process which consumes most of the resources? </p><p><strong>Lab:</strong> Let's model a simple situation to demonstrate how DTrace scripts can be 
used in the situations when no other tool is helpful. </p>
<p><strong>60 Seconds of Theory:</strong> DTrace is a big topic. We can spend weeks and months learning DTrace 
and we will still be discovering something new. The "DTrace Book" is hot off the press and available at Amazon. 
It's over 1150 pages on the subject! We'll spend about half an hour just to give you a taste of DTrace. If you need more-buy the book, find DTrace 
resources on the web--there are plenty of them. </p>
<p>In short, DTrace is a tool which allows you to look into every part of your system: 
from kernel structures and system calls to application functions, from contents of system stack to user interface. It is achieved by embedding 
a lot (literally, tens of thousands!) of probes in each and every place of the operating system. All those probes are dynamic: you can turn 
them on and off (fire them) and they don't consume system resources when not in use. </p>
<p>Another important feature of DTrace is its safety. Safety was one the central design 
goals of DTrace from its inception; that means you can (and should) use DTrace on your production systems. </p>
<p>DTrace uses a C-like scripting language called D, which supports all ANSI C operators. 
Most likely you are familiar with C language syntax, that really helps in learning D. We won't go into DTrace language and syntax here, 
we'll just try several examples that can be helpful in everyday life. </p><h3>DTrace one-liners </h3><p>There are a lot of useful one-liners in every scripting language: bash, sed, awk, Perl etc. 
DTrace is not an exception here. Try the following one-liners and see what you can observe with them. 
The usual way to work with DTrace scripts is to start them, wait for some time while DTrace is collecting data and then press Ctrl-C. </p><p>What processes are being started currently? New processes with arguments: </p>
<kbd>dtrace -n 'proc:::exec-success { trace(curpsinfo->pr_psargs); }' </kbd><p>Longer version: new processes with arguments and time </p><kbd>dtrace -qn 'syscall::exec*:return \ 
{ printf("%Y %s\n",walltimestamp,curpsinfo->pr_psargs); }' </kbd><p>Which files are being opened by each starting process? </p><kbd>dtrace -n 'syscall::open*:entry \ 
{ printf("%s %s",execname,copyinstr(arg0)); }' </kbd><p>System time is high. What are the programs doing, who is calling most of system calls? </p><kbd>dtrace -n 'syscall:::entry { @num[execname] = count(); }' </kbd><p>Which syscalls are being called most often? </p><kbd>dtrace -n 'syscall:::entry { @num[probefunc] = count(); }' </kbd><p>Which process is calling most of system calls? </p><kbd>dtrace -n 'syscall:::entry { @num[pid,execname] = count(); }' </kbd><p>Read IOPS are high. Who is reading now? Read bytes by process </p><kbd>dtrace -n 'sysinfo:::readch { @bytes[execname] = sum(arg0); }' </kbd><p>Write IOPS are high. Somebody is writing to disk, filling up all the space. Who is writing? </p><kbd>dtrace -n 'sysinfo:::writech { @bytes[execname] = sum(arg0); }' </kbd><p>How big are blocks being read? Read size distribution by process </p><kbd>dtrace -n 'sysinfo:::readch { @dist[execname] = quantize(arg0); }'</kbd> <p>Write size distribution by process </p><kbd>dtrace -n 'sysinfo:::writech { @dist[execname] = quantize(arg0); }' </kbd><p>Back to our example. Open another terminal window or a tab on your Solaris desktop. Write the following bash script: </p><pre>root@solaris:~$ <kbd>while true ; do date > /dev/null ; done</kbd> </pre><p>In your previous window/tab (where you have your root session open) run: </p><pre>root@solaris:~# <kbd>vmstat 1</kbd> </pre><p>What do you see? System time as about 70%, user time is about 30%, idle is 0%. 
System is really busy doing something in the kernel. Imagine you don't know what has happened in the other window. What would you do? </p><pre>root@solaris:~# <kbd>top</kbd></pre> <p>...shows nothing. Not a single process in the list is generating such high load. </p><pre>root@solaris:~# <kbd>prstat</kbd> </pre><p>...doesn't help as well. OK, let's think: high system time means a lot of system calls. 
Use DTrace and ask: which program generates the most of the system calls? </p><pre>root@solaris:~# <kbd>dtrace -n 'syscall:::entry { @num[execname] = count(); }'</kbd> </pre><p>Does it look cryptic to you? No worries: we have just instructed DTrace to count 
("<code>count()</code>") every system call ("<code>syscall</code>") 
when it starts ("<code>entry</code>") then aggregate that numbers of system calls by 
program name ("<code>execname</code>") and sort the output. </p><p>Wait a little bit (DTrace is collecting the data) and press Ctrl-C. You see: 
at the bottom of the list there is the "date" command. That means "date" is issuing a lot of system calls. We can try to find a program with the name "date": </p><pre>root@solaris:~# <kbd>ps -ef | grep date</kbd> </pre><p>Nothing. Let's try another script: what new processes are being executed? </p><pre>root@solaris:~# <kbd>dtrace -qn 'syscall::exec*:return \ 
{ printf("%Y %s\n",walltimestamp,curpsinfo->pr_psargs); }'</kbd> </pre><p>Now we are counting not every system call, but just exec* system calls. 
Then we instruct DTrace to output time and process arguments. </p>
<p>A-ha, now we see: a lot of "date" commands are executed each second! That's why we didn't 
see them in <code>ps -ef</code>! They just start and finish in a matter of milliseconds. But who runs all these "date" commands? Let's modify the 
last script a little bit and make it print out not only the arguments, but also the process ID and the parent process ID: </p><pre>root@solaris:~# <kbd>dtrace -qn 'syscall::exec*:return \ 
{ printf("%Y %s %d %d\n",walltimestamp,curpsinfo->pr_psargs,curpsinfo->pr_pid,curpsinfo->pr_ppid); }' </kbd></pre><p>We see a lot of strings like this: </p><pre>2011 Aug 5 20:32:24 date 28996 29016 2011 Aug 5 20:32:24 date 28997 29016 </pre><p>Now it's clear that a lot of "date" calls are generated by the 
process ID 29016! What is that process and who has started it? </p>
<pre>root@solaris:~# <kbd>ps -f -p 29016 </kbd>UID PID PPID C STIME TTY TIME CMD lab 29016 1608 12 16:27:26 pts/4 2:09 bash </pre><p>OK, now we know who's to blame! </p><p>Don't forget to kill the shell process which runs the infinite loop! </p><h2>Exercise 13: DTrace Disk </h2><p><strong>Task:</strong> You have noticed that free disk space has decreased dramatically 
and keeps decreasing very fast. Who is "eating" our disk space? </p><p>In this lab we are going to use DTrace Toolkit script. More about DTrace Toolkit in the 
tip below. You don't have to run all those scripts during the lab, leave it for your homework exercise. </p><h3>DTrace Toolkit </h3><p>Another step in learning DTrace is the excellent Toolkit written by Brendan Gregg: </p>
<pre>http://www.brendangregg.com/dtrace.html#DTraceToolkit</pre><p>The toolkit includes more than 200 very useful, well documented scripts for 
system administrator's everyday use. Here is a short description of it's content: </p>
<pre>http://www.solarisinternals.com/wiki/index.php/DTraceToolkit </pre><p>The good news is that it's already installed in the default Solaris 11 installation 
and it's located here: /usr/dtrace/DTT/</p> <p>It is highly recommended to start with the most useful scripts which are located in 
main DTT directory and are executable from command line. </p>

<p>Quote from <code>http://www.solarisinternals.com/wiki/index.php/DTraceToolkit#Top_Scripts_to_Run</code> : </p>
<p>If you have the DTraceToolkit on a misbehaving server and you don't know where to start, the following list of tools will provide the most valuable info in the shortest time: </p><p>1. <kbd>./execsnoop -v</kbd> : Look for many processes being executed quickly, as many short lived processes add considerable overhead. </p><p>2. <kbd>./iosnoop </kbd>: Watch what is happening on the disks for any unexpected activity. Check who is using the disks and the size of the disk events. </p><p>3. <kbd>./opensnoop -e</kbd> : Run <code>opensnoop</code>, learn lots. It is amazing what interesting problems <code>opensnoop</code> has unearthed. Many things are files (config files, data files, device files, libraries), and watching the open events provides a useful insight on what is really happening. </p><p>4. <kbd>./errinfo -c</kbd> : Perhaps you have a performance issue due to an application "silently failing". errinfo will show which app is experiencing what type of syscall failure, and the frequency. Of course, the app may take these errors into account and there may be no problem. Best to check anyway. </p><p>5. <kbd>./procsystime -aT</kbd> : This will show elapsed time, on-CPU time and counts for system calls. Very useful, and either -p PID or -n execname can be used to narrow examination to your target application only. </p><p>6. <kbd>./iotop -PCt8</kbd> : If disk events occured too quickly, iotop can provide a rolling summary. </p><p>7. <kbd>./rwtop -Ct8</kbd> : Rather than looking at the disk event level (<code>iosnoop</code>, <code>iotop</code>), 
<code>rwtop</code> examines at the syscall layer. This is application I/O. Much of this may be to the file system cache, some may make it to disk, and some may be for IPC. </p><p>8. <kbd>./Disk/iopattern 1 </kbd>: There are several scripts in the Disk directory for further analysis. 
<code>iopattern</code> provides several useful metrics for understanding disk access behavior. </p><p>Also don't forget to have some fun with the scripts from Misc/ directory! </p><p><strong>Lab:</strong> To simulate this problem, we are going to use a small program that does 
nothing but just eats disk space. First argument is the rate (in kB/sec) at which it will be eating space; 
second argument is a name of big file we are going to create for that. (Just in case: the program is located in <code>/home/lab/bin/.</code>) </p><pre>root@solaris:~# <kbd>diskeater 100 /export/home/lab/bigfile</kbd> </pre><p>Open another terminal window, become a "<code>root</code>" there and check free space in your system's <code>/export/home</code> directory: </p><pre>root@solaris:~# <kbd>df -b /export/home</kbd> </pre><p>Repeat this several times and take a note how your free space is decreasing. 
You can even try the following script to automate it: </p><pre>root@solaris:~# <kbd>while true ; do df -b /export/home ; sleep 1 ; done </kbd></pre><p>How can we find out which process is eating our disk space? Usually programs like <code>iostat</code> 
don't give you per-process information, only per-disk I/O workload. Try this: </p><pre>root@solaris:~# <kbd>iostat -x 5</kbd> </pre><p>What do you see? Some writing activity on one of the disks, but nothing more detailed. 
Think about other ways to solve this puzzle. Here is how it can be solved with DTrace Toolkit: </p><pre>root@solaris:~# <kbd>/usr/dtrace/DTT/rwtop</kbd> </pre><p>You will see something like this: </p><pre>
2011 Nov 15 13:11:39,  load: 0.25,  app_r:      2 KB,  app_w:    502 KB

  UID    PID   PPID CMD              D            BYTES
60004   1710   1641 nautilus         R                0
60004   1713   1641 updatemanagernot R                0
60004   1722   1641 isapython2.6     R                0
60004   1751   1641 gnome-power-mana R                0
60004   1762   1641 nwam-manager     R                0
60004   1735   1641 java             R                1
60004   1763   1641 xscreensaver     W                8
60004   1792      1 gnome-terminal   R               25
60004    810    782 Xorg             W               32
60004   1763   1641 xscreensaver     R               32
60004   1735   1641 java             W               33
60004   1792      1 gnome-terminal   W             2492
60004    810    782 Xorg             R             2532
60004   1811   1810 diskeater        W           512000</pre>

<p>Now it's pretty easy to identify who is responsible for our free space deficit! </p><p>Don't forget to stop the <code>diskeater</code> process in the other window, otherwise the problem will 
become very real (at least inside your virtual machine). </p><h2>Putting It All Together </h2><p>The whole idea of this lab is to show you some Solaris 11 features that 
can be used to create a cloud infrastructure based on Solaris. You have just created storage pools and filesystems -- think cloud storage. 
It was fast, it was simple, it was flexible. You have created and cloned Solaris zones with applications within them -- think cloud machine instances. 
You have monitored and managed zones resources -- think cloud elasticity, metering and chargeback. 
We didn't discuss in this lab Solaris network virtualization, 
Solaris security, Solaris package management and many other features 
which make Solaris 11 truly cloud-oriented operating system. Try and learn 
more about Solaris 11 features! </p>

<h2>Final Notes</h2>

<p>The virtual appliance we used in this lab is configured to be able to perform zone installation without network access. 
Namely, we've configured an internal repository with just a small subset of packages necessary for zone installations. 
If you are going to continue using this appliance with open network access, you will need to change the repository address
to Oracle's standard Solaris repository.  </p>

<pre>
root@solaris:~# <kbd>pkg set-publisher -G '*' -M '*' -g http://pkg.oracle.com/solaris/release -P solaris</kbd> 
</pre>

<h2>Further Oracle Solaris Education</h2>

<p>This Hands-on Lab is just and introduction in Oracle Solaris 11 world. We highly recommend to
continue your education with Oracle University. There is a full set of new courses covering 
Oracle Solaris 11: </p>

<ul>
<li>    Transition to Oracle Solaris 11</li>
<li>    What's New in Oracle Solaris 11</li>
<li>    What's New in Oracle Solaris 11 (Self-Study)</li>
<li>   Oracle Solaris 11 System Administration</li>
<li>    Oracle Solaris 11 Advanced System Administration</li>

</ul>

<p>Get more details at the Oracle University page: <a href="http://bit.ly/OracleSolaris11Edu">http://bit.ly/OracleSolaris11Edu</a> .</p>
<p>Good luck! </p></body>
</html>
